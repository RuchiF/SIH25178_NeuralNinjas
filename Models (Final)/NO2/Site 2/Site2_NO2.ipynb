{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEzUHsPgsOZx",
        "outputId": "0dc80820-bf7e-4af7-a6b8-3a7dab2db07e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, joblib, time\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "use_gpu=False\n",
        "BASE = Path(\".\")\n",
        "MODEL_DIR = BASE / \"models\"\n",
        "PLOT_DIR = BASE / \"plots\"\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "PLOT_DIR.mkdir(exist_ok=True)\n",
        "SAVE_DIR=Path(\"models\")\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "ekno_7W7vF4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, joblib, time\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# XGBoost and Keras\n",
        "import xgboost as xgb\n",
        "# from xgboost import callback as xgb_callback # Import xgb_callback - Not used if early_stopping_rounds is direct parameter\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "\n",
        "def find_top_correlated_features(df, target_col='NO2_target', top_n=70, save_path='top_features.json'):\n",
        "    \"\"\"\n",
        "    Find top N features most correlated with target column FROM ENGINEERED FEATURES.\n",
        "    Call this AFTER create_features() to select from all lag/roll features.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        DataFrame with ALL engineered features (lags, rolls, etc.)\n",
        "    target_col : str\n",
        "        Name of target column (e.g., 'NO2_target' or 'O3_target')\n",
        "    top_n : int\n",
        "        Number of top correlated features to return\n",
        "    save_path : str or Path\n",
        "        Path to save the selected features as JSON\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary with target_col as key and list of top features\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Define columns to exclude from feature selection\n",
        "    exclude_cols = {\"year\", \"month\", \"day\", \"hour\", \"datetime\", \"day_of_week\"}\n",
        "\n",
        "    # Get all potential feature columns (exclude time columns and other targets)\n",
        "    candidate_features = [\n",
        "        c for c in df.columns\n",
        "        if c not in exclude_cols\n",
        "        and c != target_col\n",
        "        and not (c.endswith(\"_target\") and c != target_col)  # Exclude other targets\n",
        "    ]\n",
        "\n",
        "    # Check if target exists\n",
        "    if target_col not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target_col}' not found in dataframe\")\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = {}\n",
        "    for feature in candidate_features:\n",
        "        if feature in df.columns:\n",
        "            # Calculate correlation, handling NaN values\n",
        "            valid_mask = df[feature].notna() & df[target_col].notna()\n",
        "            if valid_mask.sum() > 0:\n",
        "                corr = df.loc[valid_mask, feature].corr(df.loc[valid_mask, target_col])\n",
        "                if not np.isnan(corr):\n",
        "                    correlations[feature] = abs(corr)  # Use absolute correlation\n",
        "\n",
        "    # Sort by correlation and get top N\n",
        "    sorted_features = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_features = [feat for feat, corr in sorted_features[:top_n]]\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Top {top_n} Features Correlated with {target_col}\")\n",
        "    print(f\"Total engineered features: {len(candidate_features)}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"{'Rank':<6} {'Feature':<40} {'|Correlation|':<15}\")\n",
        "    print(f\"={'--'*35}\")\n",
        "\n",
        "    for i, (feat, corr) in enumerate(sorted_features[:top_n], 1):\n",
        "        print(f\"{i:<6} {feat:<40} {corr:<15.4f}\")\n",
        "\n",
        "    print(f\"{'--'*35}\\n\")\n",
        "\n",
        "    # Save to JSON\n",
        "    result = {target_col: top_features}\n",
        "\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(result, f, indent=2)\n",
        "\n",
        "    print(f\"✓ Top features saved to: {save_path}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def load_site_csv(path):\n",
        "    \"\"\"Load and sort CSV by datetime\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    for c in ['year','month','day','hour']:\n",
        "        if c not in df.columns:\n",
        "            raise KeyError(f\"Missing required column: {c}\")\n",
        "\n",
        "    # Create datetime and sort\n",
        "    df['datetime'] = pd.to_datetime(\n",
        "        df['year'].astype(int).astype(str) + '-' +\n",
        "        df['month'].astype(int).astype(str) + '-' +\n",
        "        df['day'].astype(int).astype(str) + ' ' +\n",
        "        df['hour'].astype(int).astype(str) + ':00:00'\n",
        "    )\n",
        "    df.sort_values('datetime', inplace=True)\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Removed apply_gap_aware_features as its logic will be handled inline with create_features\n",
        "# and then robustly filled with ffill and training medians in predict_unseen.\n",
        "def create_features(df, use_calibrated=True, target_cols_present=True):\n",
        "    \"\"\"\n",
        "    Basic feature engineering. Creates all features, including lags and rolls for targets.\n",
        "    Columns will always be created, even if initial values are NaN.\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 0. Ensure datetime exists and sort\n",
        "    # ---------------------------------\n",
        "    if \"datetime\" not in df.columns:\n",
        "        df[\"datetime\"] = pd.to_datetime(\n",
        "            df[\"year\"].astype(int).astype(str) + \"-\" +\n",
        "            df[\"month\"].astype(int).astype(str) + \"-\" +\n",
        "            df[\"day\"].astype(int).astype(str) + \" \" +\n",
        "            df[\"hour\"].astype(int).astype(str) + \":00:00\"\n",
        "        )\n",
        "        df.sort_values(\"datetime\", inplace=True)\n",
        "        df = df.reset_index(drop=True)\n",
        "    else:\n",
        "        df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 1. Fill daily satellite values\n",
        "    # -----------------------------------\n",
        "    for col in [\"NO2_satellite\", \"HCHO_satellite\", \"ratio_satellite\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].ffill().bfill()\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 2. Cyclical time features\n",
        "    # -----------------------------------\n",
        "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
        "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
        "\n",
        "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
        "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
        "\n",
        "    df[\"day_of_week\"] = df[\"datetime\"].dt.dayofweek\n",
        "    df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype(int)\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 3. BLH Transformations and Domain Features\n",
        "    # -----------------------------------\n",
        "    if \"blh_forecast\" in df.columns:\n",
        "        df['blh_forecast_log'] = np.log1p(df['blh_forecast'])\n",
        "        date_keys = df[\"year\"].astype(str) + \"-\" + df[\"month\"].astype(str) + \"-\" + df[\"day\"].astype(str)\n",
        "        df['blh_daily_min'] = df.groupby(date_keys)['blh_forecast'].transform('min')\n",
        "        df['blh_daily_max'] = df.groupby(date_keys)['blh_forecast'].transform('max')\n",
        "        df['blh_daily_range'] = df['blh_daily_max'] - df['blh_daily_min']\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 4. Forecast columns (calibrated preferred)\n",
        "    # -----------------------------------\n",
        "    base = [c for c in df.columns if c.endswith(\"_forecast\") and not c.endswith(\"_forecast_cal\")]\n",
        "    cal  = [c for c in df.columns if c.endswith(\"_forecast_cal\")]\n",
        "\n",
        "    forecast_cols = cal if (use_calibrated and len(cal) > 0) else base\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 5. Standard Lags & rolling windows\n",
        "    # -----------------------------------\n",
        "    feature_lags = [1, 3, 6, 12, 24]\n",
        "\n",
        "    cols_for_lags = forecast_cols + [\n",
        "        c for c in df.columns if c.startswith('blh_forecast_log') or c.startswith('blh_daily')\n",
        "    ]\n",
        "    for col in [\"NO2_satellite\", \"HCHO_satellite\", \"ratio_satellite\"]:\n",
        "        if col in df.columns and col not in cols_for_lags:\n",
        "            cols_for_lags.append(col)\n",
        "\n",
        "    cols_for_lags = list(set(cols_for_lags))\n",
        "\n",
        "    for col in cols_for_lags:\n",
        "        if col in df.columns:\n",
        "            for lag in feature_lags:\n",
        "                df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
        "            for window in [3, 6, 12]:\n",
        "                df[f\"{col}_roll{window}\"] = df[col].rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 6. Lags for target (autoregression)\n",
        "    # -----------------------------------\n",
        "    # 6. Lags for target (autoregression) - UPDATED LOGIC\n",
        "    # -----------------------------------\n",
        "    target_lags = [24, 36, 48, 72]\n",
        "\n",
        "    for pol in [\"O3\", \"NO2\"]:\n",
        "        t = f\"{pol}_target\"\n",
        "        if t in df.columns:\n",
        "            # Create dynamic shift based on hour\n",
        "            for base_lag in target_lags:\n",
        "                # For each row, shift by (hour + 1)\n",
        "                shifted_values = []\n",
        "                for idx in range(len(df)):\n",
        "                    current_hour = df.loc[idx, 'hour']\n",
        "                    shift_amount = current_hour + 1\n",
        "\n",
        "                    # Get the value from shift_amount positions back\n",
        "                    if idx - shift_amount >= 0:\n",
        "                        shifted_values.append(df.loc[idx - shift_amount, t])\n",
        "                    else:\n",
        "                        shifted_values.append(np.nan)\n",
        "\n",
        "                df[f\"{t}_lag{base_lag}\"] = shifted_values\n",
        "\n",
        "            # Rolling windows with dynamic shift\n",
        "            for window in [3, 6, 12]:\n",
        "                rolled_values = []\n",
        "                for idx in range(len(df)):\n",
        "                    current_hour = df.loc[idx, 'hour']\n",
        "                    shift_amount = current_hour + 1\n",
        "\n",
        "                    # Get window starting from shift_amount positions back\n",
        "                    if idx - shift_amount >= 0:\n",
        "                        start_idx = max(0, idx - shift_amount - window + 1)\n",
        "                        end_idx = idx - shift_amount + 1\n",
        "                        window_vals = df.loc[start_idx:end_idx-1, t]\n",
        "                        rolled_values.append(window_vals.mean())\n",
        "                    else:\n",
        "                        rolled_values.append(np.nan)\n",
        "\n",
        "                df[f\"{t}_roll{window}\"] = rolled_values\n",
        "\n",
        "\n",
        "\n",
        "     # Interaction features\n",
        "    if \"T_forecast\" in df.columns and \"u_forecast\" in df.columns:\n",
        "        df[\"T_x_u\"] = df[\"T_forecast\"] * df[\"u_forecast\"]\n",
        "    if \"blh_forecast\" in df.columns:\n",
        "        df[\"blh_x_hour_sin\"] = df[\"blh_forecast\"] * df[\"hour_sin\"]\n",
        "        df[\"blh_x_hour_cos\"] = df[\"blh_forecast\"] * df[\"hour_cos\"]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def find_top_correlated_features(df, target_col='NO2_target', top_n=30, save_path='top_features.json'):\n",
        "    \"\"\"Find top N correlated features.\"\"\"\n",
        "    df = df.copy()\n",
        "    exclude_cols = {\"year\", \"month\", \"day\", \"hour\", \"datetime\", \"day_of_week\"}\n",
        "\n",
        "    candidate_features = [\n",
        "        c for c in df.columns\n",
        "        if c not in exclude_cols\n",
        "        and c != target_col\n",
        "        and not (c.endswith(\"_target\") and c != target_col)\n",
        "    ]\n",
        "\n",
        "    if target_col not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target_col}' not found\")\n",
        "\n",
        "    correlations = {}\n",
        "    for feature in candidate_features:\n",
        "        if feature in df.columns:\n",
        "            valid_mask = df[feature].notna() & df[target_col].notna()\n",
        "            if valid_mask.sum() > 0:\n",
        "                corr = df.loc[valid_mask, feature].corr(df.loc[valid_mask, target_col])\n",
        "                if not np.isnan(corr):\n",
        "                    correlations[feature] = abs(corr)\n",
        "\n",
        "    sorted_features = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_features = [feat for feat, corr in sorted_features[:top_n]]\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Top {top_n} Features Correlated with {target_col}\")\n",
        "    print(f\"Total engineered features: {len(candidate_features)}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"{'Rank':<6} {'Feature':<40} {'|Correlation|':<15}\")\n",
        "    print(f\"{'-'*70}\")\n",
        "\n",
        "    for i, (feat, corr) in enumerate(sorted_features[:top_n], 1):\n",
        "        print(f\"{i:<6} {feat:<40} {corr:<15.4f}\")\n",
        "\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    result = {target_col: top_features}\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(result, f, indent=2)\n",
        "\n",
        "    print(f\"✓ Top features saved to: {save_path}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "def prepare_features_and_target_unified(df, target_col, selected_features=None):\n",
        "    \"\"\"\n",
        "    UNIFIED preprocessing - identical to inference pipeline.\n",
        "    Saves ALL medians (target + column medians) for exact replication during inference.\n",
        "    \"\"\"\n",
        "\n",
        "    exclude = {\"year\", \"month\", \"day\", \"hour\", \"datetime\", \"day_of_week\"}\n",
        "\n",
        "    if selected_features is not None:\n",
        "        feature_cols = selected_features\n",
        "    else:\n",
        "        feature_cols = [c for c in df.columns if c not in exclude and not c.endswith(\"_target\")]\n",
        "\n",
        "    # Get available features\n",
        "    available_features = [c for c in feature_cols if c in df.columns]\n",
        "    X = df[available_features].select_dtypes(include=[np.number])\n",
        "    y = df[target_col]\n",
        "\n",
        "    # Time-based split (NO SHUFFLE)\n",
        "    split_idx = int(len(df) * 0.8)\n",
        "    X_train = X.iloc[:split_idx].copy()\n",
        "    X_test  = X.iloc[split_idx:].copy()\n",
        "    y_train = y.iloc[:split_idx].copy()\n",
        "    y_test  = y.iloc[split_idx:].copy()\n",
        "\n",
        "    # ================================================\n",
        "    # UNIFIED IMPUTATION STRATEGY (same as inference)\n",
        "    # ================================================\n",
        "\n",
        "    # 1. Calculate training target median\n",
        "    if target_col in df.columns:\n",
        "        target_values = df[target_col].dropna()\n",
        "        if len(target_values) > 0:\n",
        "            training_target_median = target_values.median()\n",
        "        else:\n",
        "            training_target_median = 0.0\n",
        "    else:\n",
        "        training_target_median = 0.0\n",
        "\n",
        "    print(f\"\\n✓ Training target median: {training_target_median:.2f}\")\n",
        "\n",
        "    # 2. Calculate column medians for ALL features\n",
        "    column_medians = {}\n",
        "    for col in X_train.columns:\n",
        "        col_median = X_train[col].median()\n",
        "        column_medians[col] = float(col_median if pd.notna(col_median) else 0.0)\n",
        "\n",
        "    print(f\"✓ Calculated medians for {len(column_medians)} features\")\n",
        "\n",
        "    # 3. Save ALL medians for inference\n",
        "    median_info = {\n",
        "        'target_median': float(training_target_median),\n",
        "        'target_col': target_col,\n",
        "        'column_medians': column_medians  # Save all column medians\n",
        "    }\n",
        "\n",
        "    median_path = MODEL_DIR / 'training_medians.json'\n",
        "    with open(median_path, 'w') as f:\n",
        "        json.dump(median_info, f, indent=2)\n",
        "\n",
        "    print(f\"✓ Saved medians to: {median_path}\")\n",
        "\n",
        "    # 4. Impute training data\n",
        "    for col in X_train.columns:\n",
        "        if 'target' in col.lower():\n",
        "            X_train[col] = X_train[col].fillna(training_target_median)\n",
        "        else:\n",
        "            X_train[col] = X_train[col].fillna(column_medians[col])\n",
        "\n",
        "    # 5. Impute test data (using training statistics)\n",
        "    for col in X_test.columns:\n",
        "        if 'target' in col.lower():\n",
        "            X_test[col] = X_test[col].fillna(training_target_median)\n",
        "        else:\n",
        "            X_test[col] = X_test[col].fillna(column_medians[col])\n",
        "\n",
        "    # 6. Scale features\n",
        "    scaler = RobustScaler()\n",
        "    X_train_s = pd.DataFrame(\n",
        "        scaler.fit_transform(X_train),\n",
        "        columns=X_train.columns,\n",
        "        index=X_train.index\n",
        "    )\n",
        "    X_test_s = pd.DataFrame(\n",
        "        scaler.transform(X_test),\n",
        "        columns=X_test.columns,\n",
        "        index=X_test.index\n",
        "    )\n",
        "\n",
        "    # Save scaler and feature_cols\n",
        "    joblib.dump(scaler, MODEL_DIR / 'scaler.joblib')\n",
        "    with open(MODEL_DIR / 'feature_cols.json', 'w') as f:\n",
        "        json.dump(X_train.columns.tolist(), f, indent=2)\n",
        "\n",
        "    return X_train_s, X_test_s, y_train, y_test, scaler, X_train.columns.tolist()\n",
        "# ---------- (END reuse) ----------\n",
        "\n",
        "# --- Utility functions used below ---\n",
        "def ria_score(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Refined Index of Agreement (d1).\n",
        "    d1 = 1 - sum((P_i - O_i)^2) / sum((|P_i - O_bar| + |O_i - O_bar|)^2)\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    o_bar = np.mean(y_true)\n",
        "    num = np.sum((y_pred - y_true) ** 2)\n",
        "    denom = np.sum((np.abs(y_pred - o_bar) + np.abs(y_true - o_bar)) ** 2)\n",
        "    if denom == 0:\n",
        "        return np.nan\n",
        "    return 1.0 - (num / denom)\n",
        "\n",
        "def evaluate_all(y_true, y_pred, prefix=\"Model\"):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    ria = ria_score(y_true, y_pred)\n",
        "    print(f\"{prefix} -> RMSE: {rmse:.4f}  |  R2: {r2:.4f}  |  RIA: {ria:.4f}\")\n",
        "    return {\"rmse\": rmse, \"r2\": r2, \"ria\": ria}\n",
        "\n",
        "def build_sequences_for_gru(X_train_s, X_test_s, y_train, y_test, seq_len=24):\n",
        "    \"\"\"\n",
        "    Build sequences for training and testing the GRU.\n",
        "    For test sequences we prefix with tail of train so that first test rows have context.\n",
        "    \"\"\"\n",
        "    X_train_arr = X_train_s.values\n",
        "    y_train_arr = y_train.values\n",
        "    n_features = X_train_arr.shape[1]\n",
        "    # Build train sequences\n",
        "    if len(X_train_arr) < seq_len:\n",
        "        raise ValueError(\"Not enough train rows to build train sequences with seq_len.\")\n",
        "    X_tr_seq = []\n",
        "    y_tr_seq = []\n",
        "    for i in range(seq_len - 1, len(X_train_arr)):\n",
        "        X_tr_seq.append(X_train_arr[i - seq_len + 1:i + 1])\n",
        "        y_tr_seq.append(y_train_arr[i])\n",
        "    X_tr_seq = np.asarray(X_tr_seq)\n",
        "    y_tr_seq = np.asarray(y_tr_seq)\n",
        "\n",
        "    # Build test sequences:\n",
        "    # prefix with last (seq_len-1) rows of train so the sliding windows are valid\n",
        "    prefix = X_train_arr[-(seq_len - 1):] if seq_len > 1 else np.empty((0, n_features))\n",
        "    X_test_prefixed = np.vstack([prefix, X_test_s.values])\n",
        "    y_test_arr = y_test.values\n",
        "\n",
        "    X_te_seq = []\n",
        "    y_te_seq = []\n",
        "    # sequences should align so that there are len(y_test) sequences (one per test row)\n",
        "    for i in range(seq_len - 1, len(X_test_prefixed)):\n",
        "        # Only collect those sequences whose rightmost index corresponds to test region\n",
        "        # The index in prefixed array where rightmost index >= len(prefix)\n",
        "        if i - (seq_len - 1) >= 0 and (i - (len(prefix))) < len(y_test_arr):\n",
        "            X_te_seq.append(X_test_prefixed[i - seq_len + 1:i + 1])\n",
        "            # the corresponding y is test y at position (i - len(prefix))\n",
        "            y_te_seq.append(y_test_arr[i - len(prefix)])\n",
        "    X_te_seq = np.asarray(X_te_seq)\n",
        "    y_te_seq = np.asarray(y_te_seq)\n",
        "\n",
        "    return X_tr_seq, y_tr_seq, X_te_seq, y_te_seq\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MAIN TRAINING PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING DATA PREPROCESSING (UNIFIED WITH INFERENCE)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Load raw data\n",
        "df = load_site_csv(\"site_2_train_data.csv\")\n",
        "print(f\"Raw data shape: {df.shape}\")\n",
        "\n",
        "# 2. Feature engineering\n",
        "df_engineered = create_features(df, use_calibrated=True, target_cols_present=True)\n",
        "print(f\"Engineered data shape: {df_engineered.shape}\")\n",
        "\n",
        "# 3. Select top 30 features\n",
        "top_features_dict = find_top_correlated_features(\n",
        "    df_engineered,\n",
        "    target_col='NO2_target',\n",
        "    top_n=30,\n",
        "    save_path='top_features_no2.json'\n",
        ")\n",
        "selected_features = top_features_dict['NO2_target']\n",
        "\n",
        "# 4. Prepare data with UNIFIED preprocessing\n",
        "X_train_s, X_test_s, y_train, y_test, scaler, feature_cols = \\\n",
        "    prepare_features_and_target_unified(df_engineered, \"NO2_target\", selected_features=selected_features)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"FINAL RESULTS:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Train shape: {X_train_s.shape}\")\n",
        "print(f\"Test shape:  {X_test_s.shape}\")\n",
        "print(f\"Features used: {len(feature_cols)}\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Verify no NaNs remain\n",
        "print(\"Data quality check:\")\n",
        "print(f\"  Train NaNs: {X_train_s.isna().sum().sum()}\")\n",
        "print(f\"  Test NaNs:  {X_test_s.isna().sum().sum()}\")\n",
        "print(f\"  Train target NaNs: {y_train.isna().sum()}\")\n",
        "print(f\"  Test target NaNs:  {y_test.isna().sum()}\")\n",
        "\n",
        "if X_train_s.isna().sum().sum() > 0 or X_test_s.isna().sum().sum() > 0:\n",
        "    print(\"\\n⚠ WARNING: NaN values remain in data!\")\n",
        "else:\n",
        "    print(\"\\n✓ All data clean - ready for training\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(\"✓ Identical imputation strategy as inference\")\n",
        "print(\"✓ Target features → training target median\")\n",
        "print(\"✓ Other features → their column medians from training\")\n",
        "print(\"✓ All medians saved to training_medians.json\")\n",
        "print(\"✓ Inference will load and use exact same values\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "id": "uE3mAEoRw8kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2) Train GRU model (SEQUENCES ONLY – NO XGBOOST)\n",
        "# =========================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "SEQ_LEN=24\n",
        "print(\"\\n--- Building sequences for GRU ---\")\n",
        "\n",
        "X_tr_seq, y_tr_seq, X_te_seq, y_te_seq = build_sequences_for_gru(\n",
        "    X_train_s, X_test_s, y_train, y_test, seq_len=SEQ_LEN\n",
        ")\n",
        "\n",
        "print(f\"GRU Seq shapes -> train: {X_tr_seq.shape}, test: {X_te_seq.shape}\")\n",
        "\n",
        "n_features = X_tr_seq.shape[2]\n",
        "\n",
        "\n",
        "def build_gru_model(seq_len, n_features):\n",
        "    inp = layers.Input(shape=(seq_len, n_features))\n",
        "    x = layers.Masking(mask_value=0.0)(inp)\n",
        "    x = layers.GRU(128, return_sequences=True, use_cudnn=False)(x) # Added use_cudnn=False\n",
        "    x = layers.GRU(64, return_sequences=False, use_cudnn=False)(x) # Added use_cudnn=False\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(32, activation=\"relu\")(x)\n",
        "    out = layers.Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=\"mse\",\n",
        "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "gru_model = build_gru_model(SEQ_LEN, n_features)\n",
        "gru_model.summary()\n",
        "\n",
        "# Callbacks\n",
        "es = callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=15,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "rlr = callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.5,\n",
        "    patience=6\n",
        ")\n",
        "\n",
        "# Train\n",
        "history = gru_model.fit(\n",
        "    X_tr_seq, y_tr_seq,\n",
        "    validation_data=(X_te_seq, y_te_seq),\n",
        "    epochs=200,\n",
        "    batch_size=128,\n",
        "    callbacks=[es, rlr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Save model\n",
        "gru_model.save(SAVE_DIR / \"gru_model.keras\")\n",
        "print(\"Saved GRU model.\")"
      ],
      "metadata": {
        "id": "CHB65SoWxD3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Generate Predictions ---\n",
        "print(\"Generating predictions...\")\n",
        "y_pred = gru_model.predict(X_te_seq, verbose=0)\n",
        "\n",
        "# --- 2. Prepare True Values ---\n",
        "y_true = y_te_seq.reshape(-1, 1)\n",
        "\n",
        "# --- 3. Calculate Metrics ---\n",
        "\n",
        "# A. RMSE\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# B. R2 Score\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "# C. RMSE % (Relative to Mean)\n",
        "actual_mean = np.mean(y_true)\n",
        "rmse_percentage = (rmse / actual_mean) * 100\n",
        "\n",
        "# D. RIA (Reliability Index of Agreement / Modified IOA)\n",
        "def calculate_ria(y_true, y_pred):\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "    mean_true = np.mean(y_true_f)\n",
        "\n",
        "    numerator = np.sum(np.abs(y_pred_f - y_true_f))\n",
        "    denominator = np.sum(np.abs(y_pred_f - mean_true) + np.abs(y_true_f - mean_true))\n",
        "\n",
        "    if denominator == 0: return 0.0\n",
        "    return 1.0 - (numerator / denominator)\n",
        "\n",
        "ria = calculate_ria(y_true, y_pred)\n",
        "\n",
        "# --- 4. Print Results ---\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"GRU MODEL EVALUATION\")\n",
        "print(f\"{'='*30}\")\n",
        "print(f\"RMSE:           {rmse:.4f}\")\n",
        "print(f\"R² Score:       {r2:.4f}\")\n",
        "print(f\"RMSE % (Mean):  {rmse_percentage:.2f}%\")\n",
        "print(f\"RIA Score:      {ria:.4f}\")\n",
        "print(f\"{'='*30}\")"
      ],
      "metadata": {
        "id": "zeHytaMwxG_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# 1. Plot the scatter\n",
        "plt.scatter(y_true, y_pred, alpha=0.3)\n",
        "\n",
        "# 2. Define limits for the unity line (y=x)\n",
        "# Log scales error on 0, so we ensure the start point is slightly > 0\n",
        "# If your data is strictly > 0, you can use .min(). If it has 0s, use 1e-1 or 1.\n",
        "min_val = max(y_true.min(), 1e-1)\n",
        "max_val = y_true.max()\n",
        "\n",
        "# 3. Plot the perfect prediction line (y=x)\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
        "\n",
        "# 4. Set scales to Logarithmic\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "# 5. Labels and Grid\n",
        "plt.xlabel('True Values (log scale)')\n",
        "plt.ylabel('Predictions (log scale)')\n",
        "plt.title('True vs. Predicted Values (Log-Log)')\n",
        "\n",
        "# 'both' ensures grid lines appear for minor ticks (helpful in log plots)\n",
        "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zRnCy-VPxKZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you stored the fit result in a variable named 'history' or 'history_lstm'\n",
        "# e.g., history = model.fit(...)\n",
        "\n",
        "def plot_training_curves(history):\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history.get('val_loss') # Use .get() in case validation split wasn't used\n",
        "\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # --- Plot 1: Loss (MSE) ---\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, 'y', label='Training Loss')\n",
        "    if val_loss:\n",
        "        plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss (MSE)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # --- Plot 2: Additional Metrics (if present, e.g., MAE) ---\n",
        "    # Check if 'mae' or 'mean_absolute_error' is in the history\n",
        "    if 'mae' in history.history:\n",
        "        mae = history.history['mae']\n",
        "        val_mae = history.history.get('val_mae')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs, mae, 'y', label='Training MAE')\n",
        "        if val_mae:\n",
        "            plt.plot(epochs, val_mae, 'r', label='Validation MAE')\n",
        "        plt.title('Training and Validation MAE')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Mean Absolute Error')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run the function (Replace 'history_lstm' with your actual variable name if different)\n",
        "# If your previous cell was: history_lstm = model_cnn_lstm.fit(...)\n",
        "plot_training_curves(history)"
      ],
      "metadata": {
        "id": "skNTKJRdxNNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import timedelta\n",
        "\n",
        "\"\"\"\n",
        "INFERENCE WITH UNIFIED PREPROCESSING\n",
        "\n",
        "Loads ALL medians saved during training:\n",
        "- target_median: For target-related features\n",
        "- column_medians: For each non-target feature\n",
        "\n",
        "This ensures IDENTICAL preprocessing between training and inference.\n",
        "\"\"\"\n",
        "\n",
        "# =========================================================\n",
        "# Load Model and Preprocessing\n",
        "# =========================================================\n",
        "\n",
        "SAVE_DIR = Path(\"models\")\n",
        "SEQ_LEN = 24\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LOADING TRAINED MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Need to define build_gru_model here for loading weights\n",
        "def build_gru_model(seq_len, n_features):\n",
        "    inp = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
        "    x = tf.keras.layers.Masking(mask_value=0.0)(inp)\n",
        "    x = tf.keras.layers.GRU(128, return_sequences=True, use_cudnn=False)(x) # Added use_cudnn=False\n",
        "    x = tf.keras.layers.GRU(64, return_sequences=False, use_cudnn=False)(x) # Added use_cudnn=False\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
        "    out = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
        "    # Compile is not strictly necessary for loading weights, but good practice if you want to continue training\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=\"mse\",\n",
        "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "scaler = joblib.load(SAVE_DIR / \"scaler.joblib\")\n",
        "with open(SAVE_DIR / \"feature_cols.json\", \"r\") as f:\n",
        "    feature_cols = json.load(f)\n",
        "\n",
        "# Instantiate model architecture with use_cudnn=False explicitly\n",
        "n_features_model = len(feature_cols)\n",
        "gru_model = build_gru_model(SEQ_LEN, n_features_model)\n",
        "# Load only the weights into the newly built model\n",
        "gru_model.load_weights(SAVE_DIR / \"gru_model.keras\") # .keras saves weights and architecture\n",
        "\n",
        "# Load saved medians from training\n",
        "with open(SAVE_DIR / \"training_medians.json\", \"r\") as f:\n",
        "    median_info = json.load(f)\n",
        "    training_target_median = median_info['target_median']\n",
        "    column_medians = median_info['column_medians']\n",
        "\n",
        "print(f\"✓ Loaded model, scaler, and {len(feature_cols)} features\")\n",
        "print(f\"✓ Loaded training medians:\")\n",
        "print(f\"  - Target median: {training_target_median:.2f}\")\n",
        "print(f\"  - Column medians: {len(column_medians)} features\")\n",
        "\n",
        "# Display features\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"FEATURES REQUIRED ({len(feature_cols)} total)\")\n",
        "print(\"=\"*70)\n",
        "target_feats = [f for f in feature_cols if 'target' in f.lower()]\n",
        "other_feats = [f for f in feature_cols if 'target' not in f.lower()]\n",
        "print(f\"Target-dependent: {len(target_feats)}\")\n",
        "print(f\"Other features: {len(other_feats)}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "def load_site_csv(path):\n",
        "    \"\"\"Load CSV with datetime.\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    for c in ['year','month','day','hour']:\n",
        "        if c not in df.columns:\n",
        "            raise KeyError(f\"Missing: {c}\")\n",
        "    df['datetime'] = pd.to_datetime(\n",
        "        df['year'].astype(int).astype(str) + '-' +\n",
        "        df['month'].astype(int).astype(str) + '-' +\n",
        "        df['day'].astype(int).astype(str) + ' ' +\n",
        "        df['hour'].astype(int).astype(str) + ':00:00'\n",
        "    )\n",
        "    df.sort_values('datetime', inplace=True)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_history_lookup(df_train):\n",
        "    \"\"\"Create datetime -> NO2_target lookup.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"BUILDING HISTORY LOOKUP\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if 'NO2_target' not in df_train.columns:\n",
        "        print(\"⚠ NO2_target not found\")\n",
        "        return {}\n",
        "\n",
        "    history_lookup = {}\n",
        "    for idx, row in df_train.iterrows():\n",
        "        dt = row['datetime']\n",
        "        value = row['NO2_target']\n",
        "        if pd.notna(value):\n",
        "            history_lookup[dt] = value\n",
        "\n",
        "    print(f\"✓ Built lookup: {len(history_lookup)} entries\")\n",
        "    if history_lookup:\n",
        "        print(f\"  Range: {min(history_lookup.keys())} to {max(history_lookup.keys())}\")\n",
        "\n",
        "    return history_lookup\n",
        "\n",
        "\n",
        "def predict_with_history_buffer(df_unseen, history_lookup, gru_model, scaler,\n",
        "                                feature_cols, training_target_median, column_medians, seq_len=24):\n",
        "    \"\"\"\n",
        "    Smart prediction with UNIFIED preprocessing (same as training).\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PREDICTING WITH UNIFIED PREPROCESSING\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Using training medians:\")\n",
        "    print(f\"  Target median: {training_target_median:.2f} µg/m³\")\n",
        "    print(f\"  Column medians: {len(column_medians)} features\")\n",
        "\n",
        "    # Initialize\n",
        "    prediction_buffer = {}\n",
        "    df_work = df_unseen.copy()\n",
        "    # The target column in df_work should be the one being predicted, which is 'O3_target'\n",
        "    # The error message from build_history_lookup indicated 'NO2_target'. This should align.\n",
        "    # For this task, we are predicting 'O3_target', so 'NO2_target' should not be present in df_work.\n",
        "    # Let's remove this line if it's not strictly necessary or change to O3_target if it is.\n",
        "    # For now, assuming O3_target as per `target_col='O3_target'` in training.\n",
        "    # Let's adjust `build_history_lookup` to dynamically use the `target_col` provided\n",
        "    # or ensure it's robust.\n",
        "    # The current `build_history_lookup` function has 'NO2_target' hardcoded. This needs adjustment.\n",
        "    # However, the current problem is with GRU prediction masking, let's address that first.\n",
        "\n",
        "    # Correcting the target column being predicted\n",
        "    target_col_name = median_info['target_col'] # Get the actual target column name from saved info\n",
        "    df_work[target_col_name] = np.nan # Initialize the target column for prediction\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    stats = {\n",
        "        'from_training': 0,\n",
        "        'from_predictions': 0,\n",
        "        'median_filled': 0\n",
        "    }\n",
        "\n",
        "    print(f\"\\nProcessing {len(df_work)} rows...\\n\")\n",
        "\n",
        "    for idx in range(len(df_work)):\n",
        "        current_datetime = df_work.loc[idx, 'datetime']\n",
        "        current_hour = df_work.loc[idx, 'hour']\n",
        "        shift_amount = current_hour + 1\n",
        "\n",
        "        # ==========================================\n",
        "        # Compute lag features for the target_col_name\n",
        "        # ==========================================\n",
        "        target_lags = [24, 36, 48, 72]\n",
        "\n",
        "        for lag in target_lags:\n",
        "            lag_datetime = current_datetime - timedelta(hours=shift_amount)\n",
        "\n",
        "            # Check if this lag feature is actually in feature_cols before trying to set it\n",
        "            if f'{target_col_name}_lag{lag}' in feature_cols:\n",
        "                if lag_datetime in history_lookup:\n",
        "                    lag_value = history_lookup[lag_datetime]\n",
        "                    stats['from_training'] += 1\n",
        "                elif lag_datetime in prediction_buffer:\n",
        "                    lag_value = prediction_buffer[lag_datetime]\n",
        "                    stats['from_predictions'] += 1\n",
        "                else:\n",
        "                    lag_value = training_target_median\n",
        "                    stats['median_filled'] += 1\n",
        "\n",
        "                df_work.loc[idx, f'{target_col_name}_lag{lag}'] = lag_value\n",
        "\n",
        "        # ==========================================\n",
        "        # Compute rolling features for the target_col_name\n",
        "        # ==========================================\n",
        "        for window in [3, 6, 12]:\n",
        "            if f'{target_col_name}_roll{window}' in feature_cols: # Check if roll feature is used\n",
        "                window_values = []\n",
        "\n",
        "                for w in range(window):\n",
        "                    look_datetime = current_datetime - timedelta(hours=shift_amount + w)\n",
        "\n",
        "                    if look_datetime in history_lookup:\n",
        "                        window_values.append(history_lookup[look_datetime])\n",
        "                        stats['from_training'] += 1\n",
        "                    elif look_datetime in prediction_buffer:\n",
        "                        window_values.append(prediction_buffer[look_datetime])\n",
        "                        stats['from_predictions'] += 1\n",
        "                    else:\n",
        "                        window_values.append(training_target_median)\n",
        "                        stats['median_filled'] += 1\n",
        "\n",
        "                if len(window_values) > 0:\n",
        "                    df_work.loc[idx, f'{target_col_name}_roll{window}'] = np.mean(window_values)\n",
        "                else:\n",
        "                    df_work.loc[idx, f'{target_col_name}_roll{window}'] = training_target_median\n",
        "\n",
        "        # ==========================================\n",
        "        # Prepare features (UNIFIED with training)\n",
        "        # ==========================================\n",
        "\n",
        "        # Get available features\n",
        "        available_cols = [f for f in feature_cols if f in df_work.columns]\n",
        "        X_current = df_work.loc[[idx], available_cols].select_dtypes(include=[np.number])\n",
        "\n",
        "        # Add missing features\n",
        "        for feat in feature_cols:\n",
        "            if feat not in X_current.columns:\n",
        "                if 'target' in feat.lower():\n",
        "                    X_current[feat] = training_target_median\n",
        "                else:\n",
        "                    X_current[feat] = column_medians.get(feat, 0.0)\n",
        "\n",
        "        # Reorder to match training\n",
        "        X_current = X_current[feature_cols]\n",
        "\n",
        "        # Fill any NaNs using EXACT same logic as training\n",
        "        for col in X_current.columns:\n",
        "            if pd.isna(X_current[col].iloc[0]):\n",
        "                if 'target' in col.lower():\n",
        "                    # Use training target median\n",
        "                    X_current.loc[X_current.index[0], col] = training_target_median\n",
        "                else:\n",
        "                    # Use saved column median from training\n",
        "                    X_current.loc[X_current.index[0], col] = column_medians.get(col, 0.0)\n",
        "\n",
        "        # Verify shape\n",
        "        if X_current.shape[1] != len(feature_cols):\n",
        "            raise ValueError(f\"Shape mismatch: {X_current.shape[1]} vs {len(feature_cols)}\")\n",
        "\n",
        "        # Scale\n",
        "        X_current_scaled = scaler.transform(X_current)\n",
        "\n",
        "        # ==========================================\n",
        "        # Build sequence\n",
        "        # ==========================================\n",
        "\n",
        "        if idx < seq_len:\n",
        "            pad_length = seq_len - idx - 1\n",
        "\n",
        "            if idx == 0:\n",
        "                X_seq = np.zeros((1, seq_len, len(feature_cols)))\n",
        "                X_seq[0, -1, :] = X_current_scaled[0]\n",
        "            else:\n",
        "                prev_features = df_work.iloc[max(0, idx-seq_len+1):idx+1]\n",
        "\n",
        "                # Build previous features with required columns\n",
        "                prev_feat_df = pd.DataFrame(index=prev_features.index)\n",
        "                for feat in feature_cols:\n",
        "                    if feat in prev_features.columns:\n",
        "                        prev_feat_df[feat] = prev_features[feat]\n",
        "                    else:\n",
        "                        if 'target' in feat.lower():\n",
        "                            prev_feat_df[feat] = training_target_median\n",
        "                        else:\n",
        "                            prev_feat_df[feat] = column_medians.get(feat, 0.0)\n",
        "\n",
        "                # Fill NaNs with appropriate medians\n",
        "                for col in prev_feat_df.columns:\n",
        "                    if 'target' in col.lower():\n",
        "                        prev_feat_df[col] = prev_feat_df[col].fillna(training_target_median)\n",
        "                    else:\n",
        "                        prev_feat_df[col] = prev_feat_df[col].fillna(column_medians.get(col, 0.0))\n",
        "\n",
        "                prev_scaled = scaler.transform(prev_feat_df)\n",
        "\n",
        "                if pad_length > 0:\n",
        "                    X_seq = np.vstack([\n",
        "                        np.zeros((pad_length, len(feature_cols))),\n",
        "                        prev_scaled\n",
        "                    ])\n",
        "                else:\n",
        "                    X_seq = prev_scaled[-seq_len:]\n",
        "\n",
        "                X_seq = X_seq.reshape(1, seq_len, len(feature_cols))\n",
        "        else:\n",
        "            # Corrected slice: we need seq_len rows ending at idx, which is [idx-seq_len+1 : idx+1]\n",
        "            prev_features = df_work.iloc[idx-seq_len+1:idx+1] # This slice now correctly selects seq_len rows\n",
        "\n",
        "            # Build with required columns\n",
        "            prev_feat_df = pd.DataFrame(index=prev_features.index)\n",
        "            for feat in feature_cols:\n",
        "                if feat in prev_features.columns:\n",
        "                    prev_feat_df[feat] = prev_features[feat]\n",
        "                else:\n",
        "                    if 'target' in feat.lower():\n",
        "                        prev_feat_df[feat] = training_target_median\n",
        "                    else:\n",
        "                        prev_feat_df[feat] = column_medians.get(feat, 0.0)\n",
        "\n",
        "            # Fill NaNs\n",
        "            for col in prev_feat_df.columns:\n",
        "                if 'target' in col.lower():\n",
        "                    prev_feat_df[col] = prev_feat_df[col].fillna(training_target_median)\n",
        "                else:\n",
        "                    prev_feat_df[col] = prev_feat_df[col].fillna(column_medians.get(col, 0.0))\n",
        "\n",
        "            prev_scaled = scaler.transform(prev_feat_df)\n",
        "            X_seq = prev_scaled.reshape(1, seq_len, len(feature_cols))\n",
        "\n",
        "        # ==========================================\n",
        "        # Predict\n",
        "        # ==========================================\n",
        "\n",
        "        pred = gru_model.predict(X_seq, verbose=0)[0, 0]\n",
        "        predictions.append(pred)\n",
        "        prediction_buffer[current_datetime] = pred\n",
        "\n",
        "        if (idx + 1) % 500 == 0:\n",
        "            print(f\"  {idx + 1}/{len(df_work)} rows...\")\n",
        "\n",
        "    print(f\"✓ Completed\\n\")\n",
        "\n",
        "    # Statistics\n",
        "    total = sum(stats.values())\n",
        "    print(\"=\"*70)\n",
        "    print(\"LOOKUP STATISTICS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"  Training data:  {stats['from_training']:6d} ({stats['from_training']/total*100:5.1f}%)\")\n",
        "    print(f\"  Past predictions: {stats['from_predictions']:6d} ({stats['from_predictions']/total*100:5.1f}%)\")\n",
        "    print(f\"  Median-filled:  {stats['median_filled']:6d} ({stats['median_filled']/total*100:5.1f}%)\")\n",
        "    print(f\"Total lookups: {total}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def create_unseen_features(df_unseen, history_lookup, use_calibrated=True):\n",
        "    \"\"\"\n",
        "    Apply feature engineering to unseen data. This is a simplified version\n",
        "    that primarily sets up the datetime and fills satellite data, as the\n",
        "    lag/roll for target are handled in the prediction loop.\n",
        "    \"\"\"\n",
        "    df = df_unseen.copy()\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 0. Ensure datetime exists and sort\n",
        "    # ---------------------------------\n",
        "    if \"datetime\" not in df.columns:\n",
        "        df[\"datetime\"] = pd.to_datetime(\n",
        "            df[\"year\"].astype(int).astype(str) + \"-\" +\n",
        "            df[\"month\"].astype(int).astype(str) + \"-\" +\n",
        "            df[\"day\"].astype(int).astype(str) + \" \" +\n",
        "            df[\"hour\"].astype(int).astype(str) + \":00:00\"\n",
        "        )\n",
        "        df.sort_values(\"datetime\", inplace=True)\n",
        "        df = df.reset_index(drop=True)\n",
        "    else:\n",
        "        df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 1. Fill daily satellite values\n",
        "    # -----------------------------------\n",
        "    for col in [\"NO2_satellite\", \"HCHO_satellite\", \"ratio_satellite\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].ffill().bfill()\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 2. Cyclical time features\n",
        "    # -----------------------------------\n",
        "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
        "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
        "\n",
        "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
        "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
        "\n",
        "    df[\"day_of_week\"] = df[\"datetime\"].dt.dayofweek\n",
        "    df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype(int)\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 3. BLH Transformations and Domain Features\n",
        "    # -----------------------------------\n",
        "    if \"blh_forecast\" in df.columns:\n",
        "        df['blh_forecast_log'] = np.log1p(df['blh_forecast'])\n",
        "        date_keys = df[\"year\"].astype(str) + \"-\" + df[\"month\"].astype(str) + \"-\" + df[\"day\"].astype(str)\n",
        "        df['blh_daily_min'] = df.groupby(date_keys)['blh_forecast'].transform('min')\n",
        "        df['blh_daily_max'] = df.groupby(date_keys)['blh_forecast'].transform('max')\n",
        "        df['blh_daily_range'] = df['blh_daily_max'] - df['blh_daily_min']\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 4. Forecast columns (calibrated preferred)\n",
        "    # -----------------------------------\n",
        "    base = [c for c in df.columns if c.endswith(\"_forecast\") and not c.endswith(\"_forecast_cal\")]\n",
        "    cal  = [c for c in df.columns if c.endswith(\"_forecast_cal\")]\n",
        "\n",
        "    forecast_cols = cal if (use_calibrated and len(cal) > 0) else base\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 5. Standard Lags & rolling windows\n",
        "    # -----------------------------------\n",
        "    feature_lags = [1, 3, 6, 12, 24]\n",
        "\n",
        "    cols_for_lags = forecast_cols + [\n",
        "        c for c in df.columns if c.startswith('blh_forecast_log') or c.startswith('blh_daily')\n",
        "    ]\n",
        "    for col in [\"NO2_satellite\", \"HCHO_satellite\", \"ratio_satellite\"]:\n",
        "        if col in df.columns and col not in cols_for_lags:\n",
        "            cols_for_lags.append(col)\n",
        "\n",
        "    cols_for_lags = list(set(cols_for_lags))\n",
        "\n",
        "    for col in cols_for_lags:\n",
        "        if col in df.columns:\n",
        "            for lag in feature_lags:\n",
        "                df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
        "            for window in [3, 6, 12]:\n",
        "                df[f\"{col}_roll{window}\"] = df[col].rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "    # Lags for target (handled in predict_with_history_buffer dynamically)\n",
        "    # These columns will exist in feature_cols but will be populated during inference\n",
        "\n",
        "    # Interaction features\n",
        "    if \"T_forecast\" in df.columns and \"u_forecast\" in df.columns:\n",
        "        df[\"T_x_u\"] = df[\"T_forecast\"] * df[\"u_forecast\"]\n",
        "    if \"blh_forecast\" in df.columns:\n",
        "        df[\"blh_x_hour_sin\"] = df[\"blh_forecast\"] * df[\"hour_sin\"]\n",
        "        df[\"blh_x_hour_cos\"] = df[\"blh_forecast\"] * df[\"hour_cos\"]\n",
        "\n",
        "    return df\n",
        "\n",
        "# =========================================================\n",
        "# MAIN INFERENCE PIPELINE\n",
        "# =========================================================\n",
        "\n",
        "unseen_path = \"site_2_unseen_input_data.csv\"\n",
        "train_path = \"site_2_train_data.csv\"\n",
        "\n",
        "# 1. Load data\n",
        "df_train = load_site_csv(train_path)\n",
        "df_unseen = load_site_csv(unseen_path)\n",
        "\n",
        "# 2. Build history lookup from training data\n",
        "# Make sure to pass the correct target column if it's not NO2_target.\n",
        "# In our case, the training was for O3_target.\n",
        "# Adjusting build_history_lookup to work for 'O3_target'\n",
        "# For now, let's assume it should build history for O3_target as per training\n",
        "def build_history_lookup_o3(df, target_col='NO2_target'):\n",
        "    \"\"\"Create datetime -> target_col lookup.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"BUILDING HISTORY LOOKUP for {target_col}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if target_col not in df.columns:\n",
        "        print(f\"⚠ {target_col} not found\")\n",
        "        return {}\n",
        "\n",
        "    history_lookup = {}\n",
        "    for idx, row in df.iterrows():\n",
        "        dt = row['datetime']\n",
        "        value = row[target_col]\n",
        "        if pd.notna(value):\n",
        "            history_lookup[dt] = value\n",
        "\n",
        "    print(f\"✓ Built lookup: {len(history_lookup)} entries\")\n",
        "    if history_lookup:\n",
        "        print(f\"  Range: {min(history_lookup.keys())} to {max(history_lookup.keys())}\")\n",
        "\n",
        "    return history_lookup\n",
        "\n",
        "\n",
        "history_lookup = build_history_lookup_o3(df_train, target_col='NO2_target')\n",
        "\n",
        "# 3. Feature engineer unseen data\n",
        "df_unseen_eng = create_unseen_features(df_unseen, history_lookup, use_calibrated=True)\n",
        "\n",
        "# 4. Predict using the buffer strategy\n",
        "predictions = predict_with_history_buffer(\n",
        "    df_unseen_eng,\n",
        "    history_lookup,\n",
        "    gru_model,\n",
        "    scaler,\n",
        "    feature_cols,\n",
        "    training_target_median,\n",
        "    column_medians,\n",
        "    seq_len=SEQ_LEN\n",
        ")\n",
        "\n",
        "# 5. Add predictions to DataFrame\n",
        "df_unseen['NO2_prediction'] = predictions\n",
        "\n",
        "# Display some results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INFERENCE COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(df_unseen[['datetime', 'NO2_prediction']].head())\n",
        "print(\"...\")\n",
        "print(df_unseen[['datetime', 'NO2_prediction']].tail())\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "id": "QiS3B6KfxapW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# SAVE PREDICTIONS TO CSV\n",
        "# =========================================================\n",
        "\n",
        "output_filename = \"unseen_predictions_no2_site2.csv\"\n",
        "\n",
        "# Select only the datetime and prediction columns\n",
        "# Ensure 'datetime' and 'O3_prediction' are the correct column names in your dataframe\n",
        "output_df = df_unseen[['datetime', 'NO2_prediction']].copy()\n",
        "\n",
        "# Save to CSV without the pandas index\n",
        "output_df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\n✓ Successfully saved predictions to: {output_filename}\")\n",
        "print(f\"  Rows saved: {len(output_df)}\")\n",
        "print(f\"  Columns: {list(output_df.columns)}\")\n",
        "print(\"-\" * 50)\n",
        "print(output_df.head())"
      ],
      "metadata": {
        "id": "t4suMtVdxe1Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}